{"cells":[{"source":["import time\n","import re\n","import os\n","import bs4 \n","import json\n","import requests\n","from pandas import read_html\n","from requests.exceptions import ConnectionError\n","from tqdm import tqdm\n","from pprint import pprint\n","# from functools import reduce"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["txt = lambda elem: elem.text.replace('\\n', '').strip()\n","\n","def extract_metadata(soup):\n","    content = soup.find('div', 'contentBox')\n","    sub = content.find('p', 'drug-subtitle')\n","    if sub is None:\n","        return {'meta' : 'NO METADATA FOUND'}\n","    keys = list(map(txt, sub.find_all('b')))\n","    sub = sub.get_text()\n","    for key in keys:\n","        sub = sub.replace(key, '<key>')\n","    sub = [i.strip() for i in sub.split('<key>') if i != '']\n","    return dict(zip(keys, sub))\n","\n","\n","def extract_side_meta(soup):\n","    sidebar_meta = {}\n","    sidebar = soup.find('div', id='sidebar')\n","    sections =  ['sideBoxDrugManufacturers', 'sideBoxDrugClass', 'sideBoxRelatedDrugs']\n","    for section in sections:\n","        section = sidebar.find('div', section)\n","        if section is not None:\n","            title = txt(section.find('div', 'sideBoxTitle'))\n","            vals = list(map(txt, section.find_all('a')))\n","            # print(vals)\n","            sidebar_meta.update({title:vals})\n","    ratings = sidebar.find('div', 'drug-rating')\n","    try:\n","        sidebar_meta['User Rating and Reviews'] = {\n","            'rating': ratings.find('span', 'rating-score').text,\n","            'reviews' : ratings.find('span', 'ratings-total').text\n","            }\n","    except AttributeError:\n","        pass\n","    try:\n","        sidebar_meta['img'] = sidebar.find('div', id='drug-imprint-primary').find('img')['src']\n","    except AttributeError:\n","        pass\n","    return sidebar_meta\n","\n","def extract_text(soup, content_box=True):\n","    content = {}\n","    stack = []\n","    level = 1\n","    pointer = content\n","    prev_elem = None\n","    content_soup = soup\n","    if content_box:\n","        content_soup = soup.find('div', 'contentBox')\n","    \n","    for elem in content_soup.children:\n","        tag = str(elem.name)\n","        pointer = content\n","        if re.match('h[2-6]', tag):\n","            diff = int(tag[-1]) - level\n","            if diff:\n","                if diff < 0:\n","                    for i in range(abs(diff)):\n","                        stack.pop()\n","                    stack.pop()\n","                    stack.append(txt(elem))\n","                    prev_elem = elem\n","                if diff > 0:\n","                    for i in range(abs(diff)):\n","                        stack.append(txt(elem))\n","                        prev_elem = elem\n","                level += diff\n","            else:\n","                stack.pop()\n","                stack.append(txt(elem))\n","\n","            for point in stack:\n","                try:\n","                    pointer = pointer[point]\n","                except KeyError:\n","                    pointer[point] = {'text' : []}\n","                    pointer = pointer[point]\n","\n","        elif len(stack) and str(type(elem)) == '<class \\'bs4.element.Tag\\'>' and str(elem.name) == 'p':\n","            for point in stack:\n","                pointer = pointer[point]\n","            pointer['text'].append(txt(elem))\n","            prev_elem = elem\n","\n","        elif len(stack) and str(type(elem)) == '<class \\'bs4.element.Tag\\'>' and str(elem.name) == 'ul':\n","            for point in stack:\n","                pointer = pointer[point]\n","            lis = []\n","            for li in elem.find_all('li'):\n","                lis.append(li.get_text().replace('\\n', ' '))\n","            if prev_elem.name == 'p':\n","                try:\n","                    pointer['text'].pop()\n","                except IndexError:\n","                    pass\n","                pointer['text'].append({txt(prev_elem) : lis})\n","            elif prev_elem.name[0] == 'h':\n","                pointer['text'].append(lis)\n","            prev_elem = elem\n","    return content\n","\n","def extract_reviews(soup):\n","    def get_comments(soup, reviews, condition):\n","        comments = soup.find_all('div', 'ddc-comment')\n","        for comment in comments: \n","            review = {\n","                'user'              : comment.find('span', 'user-name'),\n","                'time_on_medication': comment.find('span', 'text-color-muted', string=re.compile('Taken for')),\n","                'comment_date'      : comment.find('span', 'comment-date'),\n","                'content'           : comment.find('p', 'ddc-comment-content'),\n","                'rating'            : comment.find('div', 'rating-score')\n","            }\n","            try:\n","                isthere = reviews[condition]\n","            except KeyError:\n","                reviews[condition] = []\n","            finally:\n","                for key in review:\n","                    try:\n","                        review[key] = txt(review[key])\n","                    except AttributeError:\n","                        review[key] = ''\n","                reviews[condition].append(review)\n","    reviews = {}\n","    conditions = soup.find('table', 'data-list')\n","    if conditions is None:\n","        get_comments(soup, reviews, 'reviews')\n","    else:\n","        condition_pages = conditions.find_all('a', string=re.compile('[0-9]* review'))\n","        conditions = read_html(str(conditions))[0]['Condition'][:-1]\n","        for i, condition in enumerate(conditions):\n","            page = 1\n","            while True:\n","                page_link = url(condition_pages[i]['href'] + f'?page={page}')\n","                r = requests.get(page_link)\n","                soup = bs4.BeautifulSoup(r.content, 'lxml')\n","                # print(page, ''.join(re.findall('[0-9]', soup.find('h1').text)))\n","                if page > 1 and ''.join(re.findall('[0-9]', soup.find('h1').text)) != str(page):\n","                    # print('page end at ', page)\n","                    break\n","                else:\n","                    get_comments(soup, reviews, condition)\n","        \n","                page+=1\n","    return reviews\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# overview_url  = lambda drug : f'https://www.drugs.com/mtm/{str(drug)}.html'\n","# monograph_url = lambda drug : f'https://www.drugs.com/monograph/{str(drug)}.html'\n","# sideeffects_url = lambda drug: f'https://www.drugs.com/sfx/{str(drug)}-side-effects.html'\n","# professional_url_0 = lambda drug : f'https://www.drugs.com/pro/{str(drug)}.html'\n","# professional_url_1 = lambda drug : f'https://www.drugs.com/ppa/{str(drug)}.html'\n","# interactions_url = lambda drug : f'https://www.drugs.com/drug-interactions/{str(drug)}.html'\n","# dosage_url = lambda drug : f'https://www.drugs.com/dosage/{str(drug)}.html'\n","\n","url = lambda link : f'https://www.drugs.com{link}'\n","\n","done = os.listdir('data/')\n","print(len(done), 'drugs already scraped')"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["drugs = {}\n","with open('alpha.json', 'r') as f:\n","    drugs = json.load(f)\n","\n","i, last, goto = 0, None, None\n","with open('pickles.txt', 'r') as f:\n","    pickles = f.read()\n","    if len(pickles) > 1:\n","        goto = pickles\n","        print('starting at', goto)\n","    else:\n","        print('found nothing pickled')\n","\n","t1 = time.time()\n","while True:\n","    try:    it \n","        for drug, link in tqdm([i for key in drugs for i in drugs[key]]):\n","            drug = drug.replace('/', ' ')\n","            last = drug; i += 1\n","            if goto is not None:\n","                if goto != last:\n","                    continue\n","                else:\n","                    goto = None\n","            # if drug in done:\n","            #     # print('skipping', drug)\n","            #     continue\n","            # print('Scraping', drug, '...') # stdout log?\n","            try:\n","                os.mkdir(f'data/{drug}')\n","            except FileExistsError:\n","                pass\n","    \n","            r = requests.get(url(link))\n","            soup = bs4.BeautifulSoup(r.content, 'lxml')\n","\n","            #  metadata\n","            with open(f'data/{drug}/{drug}_meta.json', 'w') as f:\n","                content = extract_metadata(soup)\n","                content.update(extract_side_meta(soup))\n","                json.dump(content, f, indent=2)\n","            \n","            # overview text\n","            with open(f'data/{drug}/{drug}.json', 'w') as f:\n","                content = extract_text(soup)\n","                json.dump(content, f, indent=5)\n","\n","            tabs_ul = soup.find('ul', 'nav-tabs nav-tabs-collapse vmig')\n","            if tabs_ul is not None:\n","                links = {\n","                    'sfx'       : tabs_ul.find('a', text='Side Effects'),\n","                    'dose'      : tabs_ul.find('a', text='Dosage'),\n","                    'pro'       : tabs_ul.find('a', text='Professional'),\n","                    'inter'     : tabs_ul.find('a', text='Interactions'),\n","                    'reviews'   : soup.find('p', 'user-reviews-title').find('a')\n","                }\n","                # sideffects page\n","                if links['sfx'] is not None:\n","                    r = requests.get(url(links['sfx']['href']))\n","                    soup = bs4.BeautifulSoup(r.content, 'lxml')\n","                    with open(f'data/{drug}/{drug}_sfx.json', 'w') as f:\n","                        content = extract_text(soup)\n","                        json.dump(content, f, indent=5)\n","                \n","                # professional page\n","                if links['pro'] is not None:\n","                    r = requests.get(url(links['pro']['href']))\n","                    soup = bs4.BeautifulSoup(r.content, 'lxml') \n","                    with open(f'data/{drug}/{drug}_pro.json', 'w') as f:\n","                        content = extract_text(soup)\n","                        json.dump(content, f, indent=5)\n","                    \n","                # dosage page\n","                if  links['dose'] is not None:\n","                    r = requests.get(url(links['dose']['href']))\n","                    soup = bs4.BeautifulSoup(r.content, 'lxml') \n","                    with open(f'data/{drug}/{drug}_dose.json', 'w') as f:\n","                        content = {}\n","                        for div in soup.find_all('div', 'Section'):\n","                            content.update(extract_text(div, content_box=False))\n","                        json.dump(content, f, indent=5)\n","\n","                # interactions page\n","                if links['inter'] is not None:\n","                    r = requests.get(url(links['inter']['href']))\n","                    soup = bs4.BeautifulSoup(r.content, 'lxml') \n","                    with open(f'data/{drug}/{drug}_inter.json', 'w') as f:\n","                        content = extract_text(soup)\n","                        json.dump(content, f, indent=5)\n","\n","                # reviews \n","                if links['reviews'] is not None:\n","                    r = requests.get(url(links['reviews']['href']))\n","                    soup = bs4.BeautifulSoup(r.content, 'lxml')\n","                    with open(f'data/{drug}/{drug}_reviews.json', 'w') as f:\n","                        content = extract_reviews(soup)\n","                        json.dump(content, f, indent=4)\n","        \n","            with open('pickles.txt', 'w', encoding='utf-8') as f:\n","                f.write(last)\n","        \n","            # time.sleep(2) # god mode off\n","    except (TimeoutError, ConnectionError):\n","            print('BOT KILL, CONNECTION SEVERED')\n","            goto = last\n","            print('waiting a minute...')\n","            time.sleep(60)\n","            continue\n","    except KeyboardInterrupt:\n","        break\n","    break       \n","print('total time taken', round((time.time() - t1) / 60, 2))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}